# Network Capacity Analysis using Edmonds-Karp

## Project Overview
This project implements a Network Flow optimizer from scratch in Python. It models a logistics problem where "goods" (flow) must be transported from a source node to a sink node through a network of limited-capacity edges. The goal is to determine the Maximum Throughput (Max Flow) of the network and identify bottlenecks.

The core logic utilizes the **Edmonds-Karp algorithm**. This is a specific implementation of the Ford-Fulkerson method that uses **Breadth-First Search (BFS)** to find the shortest augmenting path in the residual graph. By iteratively finding paths and updating edge capacities, the algorithm guarantees finding the maximum global flow.

## Visualization
The project includes a custom visualization engine built with `matplotlib`. It renders the graph without external graph libraries, plotting nodes and calculating vectors for directed edges. Instead of relying on GUI windows, the program saves results as PNG images for easy review.
* **Blue Lines:** Active flow.
* **Red Lines:** Edges at full capacity (Bottlenecks).
* **Grey Lines:** Unused capacity.

## File Structure
* `flow_network.py`: Contains the `FlowNetwork` class and the Edmonds-Karp/BFS logic.
* `main.py`: The entry point for running demos, file input, and benchmarks.
* `data_generator.py`: Generates synthetic datasets in CSV format.
* `dataset_*.csv`: Static example datasets for testing.
* `test_flow.py`: Unit tests verifying the algorithm against known graph topologies.

## How to Run
* **Unit Tests:** `make test`
* **Demo:** `make demo` (Saves result to `demo_result.png`)
* **Specific File:** `./main.py file dataset_small.csv` (Saves result to `output_graph.png`)
* **Timing Analysis:** `make benchmark` (Saves plot to `benchmark_plot.png`)

## Timing Analysis
The benchmark compares execution time across five datasets ranging from 10 to 300 nodes.
* **Small (10 nodes):** < 0.001s
* **Large (300 nodes):** ~0.25s

The theoretical time complexity of Edmonds-Karp is $O(V E^2)$. The timing analysis plot generated by `make benchmark` confirms this polynomial growth. As the graph density increases, the BFS must traverse more edges, and the number of augmentations increases, resulting in a non-linear increase in execution time.

## Known Limitations
* **Visualization Layout:** The node layout engine places the Source at x=0.1 and Sink at x=0.9, scattering intermediate nodes randomly in between. For datasets larger than ~50 nodes, the visualization may become dense/cluttered, though the algorithmic calculation remains accurate.
